"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3277],{8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>s});var i=t(6540);const a={},r=i.createContext(a);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(r.Provider,{value:n},e.children)}},8922:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"TechnicalDocs/APP-intro/APP-intro","title":"Build an LLM Application","description":"This section will introduce how to quickly build an LLM application for text interaction using the existing code framework.","source":"@site/docs/TechnicalDocs/APP-intro/APP-intro.mdx","sourceDirName":"TechnicalDocs/APP-intro","slug":"/TechnicalDocs/APP-intro/","permalink":"/open-source-web/docs/TechnicalDocs/APP-intro/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/TechnicalDocs/APP-intro/APP-intro.mdx","tags":[],"version":"current","frontMatter":{},"sidebar":"TechnicalDocsSidebar","previous":{"title":"Flashing Firmware Using the Forced Download Tool","permalink":"/open-source-web/docs/TechnicalDocs/quickstart/flash"},"next":{"title":"JL Official Documentation Description","permalink":"/open-source-web/docs/TechnicalDocs/SDK-intro/JL-docs"}}');var a=t(4848),r=t(8453);const o={},s="Build an LLM Application",l={},c=[{value:"How to Implement",id:"how-to-implement",level:2},{value:"How to Build Custom LLM with UnifiedChatManager",id:"how-to-build-custom-llm-with-unifiedchatmanager",level:2},{value:"How to Set Up Custom LLM",id:"how-to-set-up-custom-llm",level:3},{value:"Create env file",id:"create-env-file",level:4},{value:"Configure environment variables",id:"configure-environment-variables",level:4},{value:"How to Call and Obtain Results",id:"how-to-call-and-obtain-results",level:3},{value:"How to Maintain Conversation Context",id:"how-to-maintain-conversation-context",level:3},{value:"How to Use Qwen Omni",id:"how-to-use-qwen-omni",level:3},{value:"How We Implement It",id:"how-we-implement-it",level:2},{value:"How to Record Audio",id:"how-to-record-audio",level:2},{value:"Using Classic Bluetooth for Voice Data Transmission or Direct Phone Microphone Recording",id:"using-classic-bluetooth-for-voice-data-transmission-or-direct-phone-microphone-recording",level:3},{value:"Using Low-Power Bluetooth for Voice Data Transmission",id:"using-low-power-bluetooth-for-voice-data-transmission",level:3},{value:"How to Implement",id:"how-to-implement-1",level:2},{value:"How to Use",id:"how-to-use",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"build-an-llm-application",children:"Build an LLM Application"})}),"\n",(0,a.jsx)(n.p,{children:"This section will introduce how to quickly build an LLM application for text interaction using the existing code framework."}),"\n",(0,a.jsx)(n.h2,{id:"how-to-implement",children:"How to Implement"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"base_llm.dart"})," is the base abstraction layer that doesn't depend on specific implementations, only defines interface standards, and reserves multimodal support (such as audio input/output), while also including UI display-related helper functions."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Defines ",(0,a.jsx)(n.code,{children:"LLMType"})," enum, including ",(0,a.jsx)(n.code,{children:"customLLM"})," and ",(0,a.jsx)(n.code,{children:"qwenOmni"})," two types."]}),"\n",(0,a.jsxs)(n.li,{children:["Implements ",(0,a.jsx)(n.code,{children:"LLMTypeExtension"})," for providing model icons, colors, and display names and other UI-related functionality."]}),"\n",(0,a.jsxs)(n.li,{children:["Defines ",(0,a.jsx)(n.code,{children:"LLMConfig"})," configuration class, supporting custom model parameters."]}),"\n",(0,a.jsxs)(n.li,{children:["Defines ",(0,a.jsx)(n.code,{children:"BaseLLM"})," abstract base class, specifying interfaces that all LLM implementations must support."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Main interfaces include:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dart",children:"initialize() // Initialize LLM (load configuration, establish connection)\r\ndispose() // Release resources\r\ncreateRequest() // Synchronous text request\r\ncreateStreamingRequest() // Streaming text request\r\ncreateStreamingRequestWithAudio() // Streaming request with audio (throws unsupported exception by default)\n"})}),"\n",(0,a.jsx)(n.p,{children:"llm_factory.dart is the factory and management layer, using singleton pattern to manage LLM instance lifecycle, intelligently selecting the most suitable LLM type based on configuration and availability, supporting LLM type switching and configuration reloading, providing LLM status query and feature detection services.\r\nKey interfaces include:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dart",children:"getCurrentLLM() // Get current LLM instance (created on demand)\r\ngetLLM() // Get specified type LLM instance\r\nswitchToLLMType() // Switch to specified LLM type\r\nreloadLLMConfig() // Reload configuration\r\nisLLMAvailable() // Check if specified LLM type is available\r\ngetAvailableLLMTypes() // Get all available LLM types\n"})}),"\n",(0,a.jsx)(n.p,{children:"custom_llm.dart is the concrete implementation class for user-defined large language models, implementing functions such as loading user configuration, creating streaming and non-streaming requests, and processing return formats.\r\nKey interfaces include:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dart",children:"initialize() // Initialize and load user configuration\r\ncreateRequest() // Non-streaming request\r\ncreateStreamingRequest() // Streaming request\n"})}),"\n",(0,a.jsx)(n.p,{children:"qwen_omni_llm.dart is the concrete implementation class for multimodal language models, implementing Alibaba Cloud Tongyi Qianwen Omni multimodal API calls, processing audio input/output (WAV format), supporting streaming text responses and streaming responses with audio, and automatically handling Base64 audio encoding/decoding.\r\nKey interfaces include:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dart",children:"createStreamingRequestWithAudio() // Audio request processing\n"})}),"\n",(0,a.jsx)(n.p,{children:"unified_chat_manager.dart is the upper-level service, providing unified chat interface, masking underlying LLM differences, automatically building input content with context, automatically managing chat session history and state, supporting persistence, loading and filtering of historical records.\r\nKey interfaces include:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dart",children:"init() // Initialize chat manager\r\ncreateStreamingRequest() // Create text streaming request\r\ncreateStreamingRequestWithAudio() // Create streaming request with audio\r\nbuildInput() // Build input with historical records\r\naddChatSession() // Add chat session\n"})}),"\n",(0,a.jsx)(n.h2,{id:"how-to-build-custom-llm-with-unifiedchatmanager",children:"How to Build Custom LLM with UnifiedChatManager"}),"\n",(0,a.jsx)(n.h3,{id:"how-to-set-up-custom-llm",children:"How to Set Up Custom LLM"}),"\n",(0,a.jsx)(n.h4,{id:"create-env-file",children:"Create env file"}),"\n",(0,a.jsx)(n.p,{children:"Create an env file in the project root directory (at the same level as pubspec.yaml):"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Execute in project root directory\r\ntouch env\n"})}),"\n",(0,a.jsx)(n.h4,{id:"configure-environment-variables",children:"Configure environment variables"}),"\n",(0,a.jsx)(n.p,{children:"Add the following configuration to the env file, replacing the example values with your actual API keys:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Custom LLM configuration (OpenAI compatible)\r\nDEFAULT_LLM_TOKEN=your_openai_api_key_here\r\nDEFAULT_LLM_URL=https://api.openai.com/v1/chat/completions\r\nDEFAULT_LLM_MODEL=gpt-4o\r\n\r\n# Alibaba Cloud DashScope API Key (for QwenOmni multimodal)\r\nDEFAULT_ALIBABA_API_KEY=your_alibaba_dashscope_api_key_here\r\n\r\n# Tencent Cloud ASR configuration (for cloud speech recognition)\r\nDEFAULT_TENCENT_SECRET_ID=your_tencent_secret_id_here\r\nDEFAULT_TENCENT_SECRET_KEY=your_tencent_secret_key_here\r\nDEFAULT_TENCENT_TOKEN=your_tencent_token_here\r\n\r\n# OpenAI TTS configuration (for speech synthesis)\r\nDEFAULT_OPENAI_TTS_BASE_URL=https://api.openai.com/v1/audio/speech\n"})}),"\n",(0,a.jsx)(n.h3,{id:"how-to-call-and-obtain-results",children:"How to Call and Obtain Results"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dart",children:'Follow the example below to make calls and create text requests:\r\n// Initialize manager\r\nfinal manager = UnifiedChatManager();\r\nawait manager.init(systemPrompt: "You are a professional assistant");\r\n\r\n// Streaming text conversation\r\nfinal stream = manager.createStreamingRequest(text: "Hello");\r\nstream.listen((response) => print(response));\r\n\r\n// Non-streaming text conversation\r\nfinal response = manager.createRequest(text: "Hello");\r\nprint(response)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"how-to-maintain-conversation-context",children:"How to Maintain Conversation Context"}),"\n",(0,a.jsx)(n.p,{children:"By default, when using createStreamingRequest to call streaming text requests, the buildInput function will be automatically called to combine all historical information into one message. buildInput reads ChatSession, so you need to manually call addChatSession to add historical sessions, as follows:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dart",children:'// Initialize manager\r\nfinal manager = UnifiedChatManager();\r\nawait manager.init(systemPrompt: "You are a professional assistant");\r\n\r\n// Add historical records\r\nmanager.addChatSession(\'user\', "Hello");\r\nmanager.addChatSession(\'assistant\', "How can I help you?");\r\n\r\n// Streaming text conversation\r\nfinal stream = manager.createStreamingRequest(text: "How\'s the weather today?");\r\nstream.listen((response) => print(response));\n'})}),"\n",(0,a.jsx)(n.h3,{id:"how-to-use-qwen-omni",children:"How to Use Qwen Omni"}),"\n",(0,a.jsx)(n.p,{children:"Follow the example below to make calls and create requests that include both audio and text:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dart",children:'// Initialize manager\r\nfinal manager = UnifiedChatManager();\r\nawait manager.init(systemPrompt: "You are a professional assistant");\r\n\r\n// Audio conversation\r\nfinal audioStream = manager.createStreamingRequestWithAudio(\r\n    audioData: audioBytes,\r\n    userMessage: text,\r\n);\r\naudioStream.listen((response) => print(response));\n'})}),"\n",(0,a.jsx)(n.p,{children:"Note: createStreamingRequestWithAudio will also automatically read ChatSession by default to build text context."}),"\n",(0,a.jsx)(n.h1,{id:"asr",children:"ASR"}),"\n",(0,a.jsx)(n.h2,{id:"how-we-implement-it",children:"How We Implement It"}),"\n",(0,a.jsx)(n.p,{children:"For classic Bluetooth, we use AudioRecorder to implement microphone recording, dependencies:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dart",children:"dependencies:\r\n    record: ^5.1.2\n"})}),"\n",(0,a.jsx)(n.p,{children:"For low-power Bluetooth, we designed a BleService class specifically for low-power Bluetooth voice data transmission. It provides low-power Bluetooth device connection, data transmission and state management, dependencies:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dart",children:"dependencies:\r\n    flutter_foreground_task: 8.13.0\r\n    flutter_blue_plus: ^1.34.5\n"})}),"\n",(0,a.jsx)(n.p,{children:"Key interfaces include:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dart",children:"init() // Initialize service, get saved deviceRemoteId, auto-reconnect if exists\r\ngetAndConnect() // Actively get deviceRemoteId and connect\r\nlistenToConnectionState() // Listen to Bluetooth connection state changes\r\nforgetDevice() // Forget device: disconnect and clear saved device information\r\ndispose() // Release resources, cancel subscriptions and close data stream\n"})}),"\n",(0,a.jsx)(n.p,{children:"Key members:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dart",children:"final StreamController<Uint8List> _dataController = StreamController<Uint8List>();\r\nStream<Uint8List> get dataStream => _dataController.stream;\r\nExternal parties obtain transmission data from low-power Bluetooth devices by listening to dataStream.\n"})}),"\n",(0,a.jsx)(n.h2,{id:"how-to-record-audio",children:"How to Record Audio"}),"\n",(0,a.jsx)(n.h3,{id:"using-classic-bluetooth-for-voice-data-transmission-or-direct-phone-microphone-recording",children:"Using Classic Bluetooth for Voice Data Transmission or Direct Phone Microphone Recording"}),"\n",(0,a.jsx)(n.p,{children:"After obtaining microphone permissions, follow the example below to turn on the microphone:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dart",children:"// Configure microphone parameters, example only\r\nconst config = RecordConfig(\r\n    encoder: AudioEncoder.pcm16bits,\r\n    sampleRate: 16000,\r\n    numChannels: 1,\r\n);\r\n\r\n// Turn on microphone\r\nfinal recorder = AudioRecorder();\r\nfinal recordStream = await recorder.startStream(config);\r\nrecordStream.listen((data) {\r\n    ...\r\n});\n"})}),"\n",(0,a.jsx)(n.h3,{id:"using-low-power-bluetooth-for-voice-data-transmission",children:"Using Low-Power Bluetooth for Voice Data Transmission"}),"\n",(0,a.jsx)(n.p,{children:"After connecting with low-power Bluetooth devices, follow the example below to listen for data:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dart",children:"await BleService().init();\r\nfinal bleDataSubscription = BleService().dataStream.listen((value) {\r\n    ...\r\n});\n"})}),"\n",(0,a.jsx)(n.h1,{id:"tts",children:"TTS"}),"\n",(0,a.jsx)(n.h2,{id:"how-to-implement-1",children:"How to Implement"}),"\n",(0,a.jsx)(n.p,{children:"Using Flutter TTS to use the phone's local text-to-speech module, dependencies:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dart",children:"dependencies:\r\n    flutter_tts: ^4.0.2\n"})}),"\n",(0,a.jsxs)(n.p,{children:["For specific implementation details, refer to the ",(0,a.jsx)(n.a,{href:"https://pub.dev/packages/flutter_tts",children:"flutter_tts official documentation"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"how-to-use",children:"How to Use"}),"\n",(0,a.jsx)(n.p,{children:"Example code is as follows:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dart",children:'// Initialize\r\nfinal flutterTts = FlutterTts();\r\nawait _flutterTts.awaitSpeakCompletion(true);\r\nif (Platform.isAndroid) {\r\n  await _flutterTts.setQueueMode(1);\r\n}\r\n\r\n// Call text-to-speech\r\nflutterTts.speak("Hello, this is a text-to-speech test.");\r\n\r\n// Interrupt or stop speech synthesis\r\nflutterTts.stop();\n'})})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);